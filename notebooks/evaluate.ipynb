{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from nervaluate import Evaluator\n",
    "\n",
    "from pie import Pie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data(file=\"data/gold.json\"):\n",
    "    \"\"\"Load test data from directory\"\"\"\n",
    "    with open(file) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "data = load_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pie = Pie()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for example in tqdm(test_data):\n",
    "    response = pie.bake(example[\"text\"])\n",
    "    example[\"predicted\"] = response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### format predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern to capture placeholders\n",
    "ENTITY_PATTERN = re.compile(r\"\\{[A-Z]+\\s?[A-Z]*\\}\")\n",
    "\n",
    "def extract_entities_from_text(example: dict) -> list[dict]:\n",
    "    \"\"\" \n",
    "    Finds the masked entities in the predicted text and extracts \n",
    "    the corresponding substrings with indices from the original text.\n",
    "    \"\"\"\n",
    "    extracted_entities = []\n",
    "\n",
    "    # Split the predicted text using the placeholders\n",
    "    chunks = ENTITY_PATTERN.split(example[\"predicted\"])\n",
    "\n",
    "    # Find the labels of the placeholders\n",
    "    labels = ENTITY_PATTERN.findall(example[\"predicted\"])\n",
    "\n",
    "    # Initiate the last index found to 0\n",
    "    last_idx = 0\n",
    "\n",
    "    for i, chunk in enumerate(chunks[:-1]):\n",
    "        \n",
    "        start_of_chunk = example[\"text\"].find(chunk, last_idx)\n",
    "        end_of_chunk = start_of_chunk + len(chunk)\n",
    "        \n",
    "        # Find the start of the next chunk in the original text\n",
    "        start_of_next_chunk = example[\"text\"].find(chunks[i + 1], end_of_chunk)\n",
    "\n",
    "        extracted_entities.append({\n",
    "            \"label\": labels[i],\n",
    "            \"start\": end_of_chunk,\n",
    "            \"end\": start_of_next_chunk,\n",
    "            \"text\": example[\"text\"][end_of_chunk:start_of_next_chunk],\n",
    "        })\n",
    "        # Update the last index to the end of the current chunk\n",
    "        last_idx = end_of_chunk\n",
    "\n",
    "    return extracted_entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example in tqdm(test_data):\n",
    "    example[\"predicted_entities\"] = extract_entities_from_text(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_date = datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "\n",
    "save_path = f\"tests/performance_tests/test_models/{pie.llm.model_name}-{str_date}.json\"\n",
    "p = Path(save_path)\n",
    "\n",
    "with p.open(\"w\", encoding=\"utf8\") as f:\n",
    "    json.dump(test_data, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run evaluation\n",
    "\n",
    "See documentation on nervaluate and how to interpret the results here:\n",
    "https://www.davidsbatista.net/blog/2018/05/09/Named_Entity_Evaluation/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use in-memory object from above or load previous model predictions on test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with p.open(\"r\", encoding=\"utf8\") as f:\n",
    "    test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = [t[\"predicted_entities\"] for t in test_data]\n",
    "true = [t[\"entities\"] for t in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(\n",
    "    true, pred,\n",
    "    tags= [\"{{NAME}}\", \"{{EMAIL}}\", \"{{PHONE}}\", \"{{CPR NUMBER}}\", \"{{ORGANIZATION}}\", \"{{LOCATION}}\", \"{{ZIP CODE}}\"]\n",
    ")\n",
    "# Returns overall metrics and metrics for each tag\n",
    "results, results_per_tag = evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "reform = {\n",
    "    (outer_key, inner_key): values for outer_key, inner_dict \n",
    "    in results_per_tag.items() for inner_key, values in inner_dict.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataf = pd.DataFrame(reform).round(2)\n",
    "dataf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a6a909a7897105fe84eddea67f522df1a05c743493a0115db02b657dd07d5bc0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
